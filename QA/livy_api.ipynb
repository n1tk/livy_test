{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Implementation of Livy API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livy_submit import livy_api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Manual QA of Livy API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the API object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_url = 'livy.demo.anaconda.com:8998'\n",
    "namenode_url = 'http://ec2-3-93-61-21.compute-1.amazonaws.com:50070'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = livy_api.LivyAPI(server_url=server_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload files to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livy_submit import hdfs_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mhdfs_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamenode_url\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mhdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkerberos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerberosClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mget_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamenode_url\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mKerberosClient\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Thin wrapper around KerberosClient\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Parameters\u001b[0m\n",
       "\u001b[0;34m    ----------\u001b[0m\n",
       "\u001b[0;34m    namenode_url: The url of the namenode. Should include protocol (http/https) and port\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mKerberosClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamenode_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/dev/livy-submit/livy_submit/hdfs_api.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hdfs_api.get_client??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_client = hdfs_api.get_client(namenode_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "HdfsError",
     "evalue": "b'<html>\\n<head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=ISO-8859-1\"/>\\n<title>Error 403 GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos credentails)</title>\\n</head>\\n<body><h2>HTTP ERROR 403</h2>\\n<p>Problem accessing /webhdfs/v1/user/edill. Reason:\\n<pre>    GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos credentails)</pre></p><hr /><i><small>Powered by Jetty://</small></i><br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n\\n</body>\\n</html>\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHdfsError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-027a6b0f3e18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhdfs_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/user/edill'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda/envs/livy-submit-dev/lib/python3.7/site-packages/hdfs/client.py\u001b[0m in \u001b[0;36mlist\u001b[0;34m(self, hdfs_path, status)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Listing %r.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdfs_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0mhdfs_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdfs_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m     \u001b[0mstatuses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_list_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdfs_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FileStatuses'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FileStatus'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m     if len(statuses) == 1 and (\n\u001b[1;32m   1009\u001b[0m       \u001b[0;32mnot\u001b[0m \u001b[0mstatuses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pathSuffix'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdfs_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'FILE'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/livy-submit-dev/lib/python3.7/site-packages/hdfs/client.py\u001b[0m in \u001b[0;36mapi_handler\u001b[0;34m(client, hdfs_path, data, strict, **params)\u001b[0m\n\u001b[1;32m    116\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'RetriableException'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'StandbyException'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHdfsError\u001b[0m: b'<html>\\n<head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=ISO-8859-1\"/>\\n<title>Error 403 GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos credentails)</title>\\n</head>\\n<body><h2>HTTP ERROR 403</h2>\\n<p>Problem accessing /webhdfs/v1/user/edill. Reason:\\n<pre>    GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos credentails)</pre></p><hr /><i><small>Powered by Jetty://</small></i><br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n<br/>                                                \\n\\n</body>\\n</html>\\n'"
     ]
    }
   ],
   "source": [
    "hdfs_client.list('/user/edill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticket cache: FILE:/tmp/krb5cc_1000\n",
      "Default principal: edill@DEMO.ANACONDA.COM\n",
      "\n",
      "Valid starting       Expires              Service principal\n",
      "09/04/2019 12:10:42  09/04/2019 22:10:42  krbtgt/DEMO.ANACONDA.COM@DEMO.ANACONDA.COM\n",
      "\trenew until 09/11/2019 12:10:41, Etype (skey, tkt): aes256-cts-hmac-sha1-96, aes256-cts-hmac-sha1-96 \n",
      "09/04/2019 16:07:13  09/04/2019 22:10:42  HTTP/ec2-3-93-61-21.compute-1.amazonaws.com@\n",
      "\trenew until 09/11/2019 12:10:41, Etype (skey, tkt): aes256-cts-hmac-sha1-96, aes256-cts-hmac-sha1-96 \n",
      "09/04/2019 16:07:13  09/04/2019 22:10:42  HTTP/ec2-3-93-61-21.compute-1.amazonaws.com@DEMO.ANACONDA.COM\n",
      "\trenew until 09/11/2019 12:10:41, Etype (skey, tkt): aes256-cts-hmac-sha1-96, aes256-cts-hmac-sha1-96 \n"
     ]
    }
   ],
   "source": [
    "!klist -e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__registry__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_append',\n",
       " '_create',\n",
       " '_delete',\n",
       " '_get_acl_status',\n",
       " '_get_content_summary',\n",
       " '_get_file_checksum',\n",
       " '_get_file_status',\n",
       " '_get_home_directory',\n",
       " '_get_trash_root',\n",
       " '_list_status',\n",
       " '_lock',\n",
       " '_mkdirs',\n",
       " '_modify_acl_entries',\n",
       " '_open',\n",
       " '_proxy',\n",
       " '_rename',\n",
       " '_request',\n",
       " '_session',\n",
       " '_set_acl',\n",
       " '_set_owner',\n",
       " '_set_permission',\n",
       " '_set_replication',\n",
       " '_set_times',\n",
       " '_timeout',\n",
       " '_urls',\n",
       " 'acl_status',\n",
       " 'checksum',\n",
       " 'content',\n",
       " 'delete',\n",
       " 'download',\n",
       " 'from_options',\n",
       " 'list',\n",
       " 'makedirs',\n",
       " 'parts',\n",
       " 'read',\n",
       " 'rename',\n",
       " 'resolve',\n",
       " 'root',\n",
       " 'set_acl',\n",
       " 'set_owner',\n",
       " 'set_permission',\n",
       " 'set_replication',\n",
       " 'set_times',\n",
       " 'status',\n",
       " 'upload',\n",
       " 'url',\n",
       " 'urls',\n",
       " 'walk',\n",
       " 'write']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(hdfs_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from hdfs.ext.kerberos import KerberosClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "namenode_connection_url = f'http://{server_url}:50070'\n",
    "run_file = os.path.abspath('pi.py')\n",
    "hdfs_dir = '/user/edill/pi.py'\n",
    "job_name = 'edill-pi'\n",
    "spark_config = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/notebooks/livy-submit-old/QA/pi.py'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. upload run_file to hdfs_dir\n",
    "client = KerberosClient(namenode_connection_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that `pi.py` exists in the home directory of your user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.sparkStaging', 'banking-leads.parquet', 'banking_leads.csv', 'pi.py']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list('/user/edill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/user/edill/pi.py'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_on_hdfs = client.upload(hdfs_path='', local_path=run_file, overwrite=True)\n",
    "file_on_hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start using the Livy Python API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what info we have available. This will likely be an output that looks like this:\n",
    "```\n",
    "(0, 0, {})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, {})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.all_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one should throw a stack trace with:\n",
    "```\n",
    "HTTPError: 404 Client Error: Not Found for url: http://ip-172-31-20-241.ec2.internal:8998/batches/26\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: http://ip-172-31-20-241.ec2.internal:8998/batches/26",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3f7854afa667>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m26\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/notebooks/livy-submit/livy_submit/livy_api.py\u001b[0m in \u001b[0;36minfo\u001b[0;34m(self, batch_id)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \"\"\"\n\u001b[1;32m     89\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%s/%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/livy-submit/livy_submit/livy_api.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, rest_action, url, data)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_auth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Make sure that our request was successful\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/livy-submit/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: http://ip-172-31-20-241.ec2.internal:8998/batches/26"
     ]
    }
   ],
   "source": [
    "api.info(26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit your livy job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'edill-pi', 'file': '/user/edill/pi.py'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batch(id=30, appId=None, appInfo={'driverLogUrl': None, 'sparkUiUrl': None}, log=[see self.log], state=starting)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job1 = api.submit(file=file_on_hdfs, name=job_name)\n",
    "job1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check on its info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(id=30, appId=None, appInfo={'driverLogUrl': None, 'sparkUiUrl': None}, log=[see self.log], state=starting)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.info(job1.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit another job so we can have additional info in the all_info function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'edill-pi', 'file': '/user/edill/pi.py'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batch(id=31, appId=None, appInfo={'driverLogUrl': None, 'sparkUiUrl': None}, log=[see self.log], state=starting)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job2 = api.submit(file=file_on_hdfs, name=job_name)\n",
    "job2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check all available batch jobs. You should have two, if you've run the cells in order and only once each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " 4,\n",
       " {28: Batch(id=28, appId=application_1544723249474_0018, appInfo={'driverLogUrl': None, 'sparkUiUrl': 'http://ip-172-31-20-241.ec2.internal:20888/proxy/application_1544723249474_0018/'}, log=[see self.log], state=success),\n",
       "  29: Batch(id=29, appId=None, appInfo={'driverLogUrl': None, 'sparkUiUrl': None}, log=[see self.log], state=starting),\n",
       "  30: Batch(id=30, appId=None, appInfo={'driverLogUrl': None, 'sparkUiUrl': None}, log=[see self.log], state=starting),\n",
       "  31: Batch(id=31, appId=None, appInfo={'driverLogUrl': None, 'sparkUiUrl': None}, log=[see self.log], state=starting)})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.all_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Look at the logs for the first job. Make sure you see this log line:\n",
    "```\n",
    "  'Pi is roughly 3.139560',\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,\n",
       " 42,\n",
       " 142,\n",
       " ['18/12/17 14:17:41 INFO Client: Submitting application application_1544723249474_0020 to ResourceManager',\n",
       "  '18/12/17 14:17:41 INFO YarnClientImpl: Submitted application application_1544723249474_0020',\n",
       "  '18/12/17 14:17:41 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1544723249474_0020 and attemptId None',\n",
       "  '18/12/17 14:17:42 INFO Client: Application report for application_1544723249474_0020 (state: ACCEPTED)',\n",
       "  '18/12/17 14:17:42 INFO Client: ',\n",
       "  '\\t client token: Token { kind: YARN_CLIENT_TOKEN, service:  }',\n",
       "  '\\t diagnostics: AM container is launched, waiting for AM container to Register with RM',\n",
       "  '\\t ApplicationMaster host: N/A',\n",
       "  '\\t ApplicationMaster RPC port: -1',\n",
       "  '\\t queue: default',\n",
       "  '\\t start time: 1545056261402',\n",
       "  '\\t final status: UNDEFINED',\n",
       "  '\\t tracking URL: http://ip-172-31-20-241.ec2.internal:20888/proxy/application_1544723249474_0020/',\n",
       "  '\\t user: edill',\n",
       "  '18/12/17 14:17:43 INFO Client: Application report for application_1544723249474_0020 (state: ACCEPTED)',\n",
       "  '18/12/17 14:17:44 INFO Client: Application report for application_1544723249474_0020 (state: ACCEPTED)',\n",
       "  '18/12/17 14:17:45 INFO Client: Application report for application_1544723249474_0020 (state: ACCEPTED)',\n",
       "  '18/12/17 14:17:46 INFO Client: Application report for application_1544723249474_0020 (state: ACCEPTED)',\n",
       "  '18/12/17 14:17:47 INFO Client: Application report for application_1544723249474_0020 (state: ACCEPTED)',\n",
       "  '18/12/17 14:17:48 INFO Client: Application report for application_1544723249474_0020 (state: ACCEPTED)',\n",
       "  '18/12/17 14:17:49 INFO Client: Application report for application_1544723249474_0020 (state: ACCEPTED)',\n",
       "  '18/12/17 14:17:50 INFO Client: Application report for application_1544723249474_0020 (state: ACCEPTED)',\n",
       "  '18/12/17 14:17:51 INFO Client: Application report for application_1544723249474_0020 (state: ACCEPTED)',\n",
       "  '18/12/17 14:17:52 INFO Client: Application report for application_1544723249474_0020 (state: ACCEPTED)',\n",
       "  '18/12/17 14:17:53 INFO Client: Application report for application_1544723249474_0020 (state: ACCEPTED)',\n",
       "  '18/12/17 14:17:54 INFO Client: Application report for application_1544723249474_0020 (state: ACCEPTED)',\n",
       "  '18/12/17 14:17:55 INFO Client: Application report for application_1544723249474_0020 (state: ACCEPTED)',\n",
       "  '18/12/17 14:17:55 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-31-20-241.ec2.internal, PROXY_URI_BASES -> http://ip-172-31-20-241.ec2.internal:20888/proxy/application_1544723249474_0020), /proxy/application_1544723249474_0020',\n",
       "  '18/12/17 14:17:55 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter',\n",
       "  '18/12/17 14:17:56 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)',\n",
       "  '18/12/17 14:17:56 INFO Client: Application report for application_1544723249474_0020 (state: RUNNING)',\n",
       "  '18/12/17 14:17:56 INFO Client: ',\n",
       "  '\\t client token: Token { kind: YARN_CLIENT_TOKEN, service:  }',\n",
       "  '\\t diagnostics: N/A',\n",
       "  '\\t ApplicationMaster host: 172.31.28.18',\n",
       "  '\\t ApplicationMaster RPC port: 0',\n",
       "  '\\t queue: default',\n",
       "  '\\t start time: 1545056261402',\n",
       "  '\\t final status: UNDEFINED',\n",
       "  '\\t tracking URL: http://ip-172-31-20-241.ec2.internal:20888/proxy/application_1544723249474_0020/',\n",
       "  '\\t user: edill',\n",
       "  '18/12/17 14:17:56 INFO YarnClientSchedulerBackend: Application application_1544723249474_0020 has started running.',\n",
       "  \"18/12/17 14:17:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41805.\",\n",
       "  '18/12/17 14:17:56 INFO NettyBlockTransferService: Server created on ip-172-31-20-241.ec2.internal:41805',\n",
       "  '18/12/17 14:17:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy',\n",
       "  '18/12/17 14:17:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-31-20-241.ec2.internal, 41805, None)',\n",
       "  '18/12/17 14:17:56 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-20-241.ec2.internal:41805 with 414.4 MB RAM, BlockManagerId(driver, ip-172-31-20-241.ec2.internal, 41805, None)',\n",
       "  '18/12/17 14:17:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-31-20-241.ec2.internal, 41805, None)',\n",
       "  '18/12/17 14:17:56 INFO BlockManager: external shuffle service port = 7337',\n",
       "  '18/12/17 14:17:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-31-20-241.ec2.internal, 41805, None)',\n",
       "  '18/12/17 14:17:57 INFO EventLoggingListener: Logging events to hdfs:/var/log/spark/apps/application_1544723249474_0020',\n",
       "  '18/12/17 14:17:57 INFO Utils: Using initial executors = 0, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances',\n",
       "  '18/12/17 14:17:57 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8',\n",
       "  '18/12/17 14:17:57 INFO SparkContext: Starting job: reduce at /mnt/tmp/spark-9106cee7-d23c-42fb-afb1-6baaf8b4585d/pi.py:48',\n",
       "  '18/12/17 14:17:57 INFO DAGScheduler: Got job 0 (reduce at /mnt/tmp/spark-9106cee7-d23c-42fb-afb1-6baaf8b4585d/pi.py:48) with 2 output partitions',\n",
       "  '18/12/17 14:17:57 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at /mnt/tmp/spark-9106cee7-d23c-42fb-afb1-6baaf8b4585d/pi.py:48)',\n",
       "  '18/12/17 14:17:57 INFO DAGScheduler: Parents of final stage: List()',\n",
       "  '18/12/17 14:17:57 INFO DAGScheduler: Missing parents: List()',\n",
       "  '18/12/17 14:17:57 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[1] at reduce at /mnt/tmp/spark-9106cee7-d23c-42fb-afb1-6baaf8b4585d/pi.py:48), which has no missing parents',\n",
       "  '18/12/17 14:17:58 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.4 KB, free 414.4 MB)',\n",
       "  '18/12/17 14:17:58 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.0 KB, free 414.4 MB)',\n",
       "  '18/12/17 14:17:58 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-20-241.ec2.internal:41805 (size: 3.0 KB, free: 414.4 MB)',\n",
       "  '18/12/17 14:17:58 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1079',\n",
       "  '18/12/17 14:17:58 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[1] at reduce at /mnt/tmp/spark-9106cee7-d23c-42fb-afb1-6baaf8b4585d/pi.py:48) (first 15 tasks are for partitions Vector(0, 1))',\n",
       "  '18/12/17 14:17:58 INFO YarnScheduler: Adding task set 0.0 with 2 tasks',\n",
       "  '18/12/17 14:17:59 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1)',\n",
       "  '18/12/17 14:18:04 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.28.18:47600) with ID 1',\n",
       "  '18/12/17 14:18:04 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 1)',\n",
       "  '18/12/17 14:18:04 WARN TaskSetManager: Stage 0 contains a task of very large size (371 KB). The maximum recommended task size is 100 KB.',\n",
       "  '18/12/17 14:18:04 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, ip-172-31-28-18.ec2.internal, executor 1, partition 0, PROCESS_LOCAL, 380245 bytes)',\n",
       "  '18/12/17 14:18:04 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, ip-172-31-28-18.ec2.internal, executor 1, partition 1, PROCESS_LOCAL, 508048 bytes)',\n",
       "  '18/12/17 14:18:04 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-28-18.ec2.internal:34597 with 2.8 GB RAM, BlockManagerId(1, ip-172-31-28-18.ec2.internal, 34597, None)',\n",
       "  '18/12/17 14:18:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-28-18.ec2.internal:34597 (size: 3.0 KB, free: 2.8 GB)',\n",
       "  '18/12/17 14:18:06 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1681 ms on ip-172-31-28-18.ec2.internal (executor 1) (1/2)',\n",
       "  '18/12/17 14:18:06 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1705 ms on ip-172-31-28-18.ec2.internal (executor 1) (2/2)',\n",
       "  '18/12/17 14:18:06 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool ',\n",
       "  '18/12/17 14:18:06 INFO DAGScheduler: ResultStage 0 (reduce at /mnt/tmp/spark-9106cee7-d23c-42fb-afb1-6baaf8b4585d/pi.py:48) finished in 8.145 s',\n",
       "  '18/12/17 14:18:06 INFO DAGScheduler: Job 0 finished: reduce at /mnt/tmp/spark-9106cee7-d23c-42fb-afb1-6baaf8b4585d/pi.py:48, took 8.225928 s',\n",
       "  'Pi is roughly 3.141880',\n",
       "  '18/12/17 14:18:06 INFO SparkUI: Stopped Spark web UI at http://ip-172-31-20-241.ec2.internal:4041',\n",
       "  '18/12/17 14:18:06 INFO YarnClientSchedulerBackend: Interrupting monitor thread',\n",
       "  '18/12/17 14:18:06 INFO YarnClientSchedulerBackend: Shutting down all executors',\n",
       "  '18/12/17 14:18:06 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down',\n",
       "  '18/12/17 14:18:06 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices',\n",
       "  '(serviceOption=None,',\n",
       "  ' services=List(),',\n",
       "  ' started=false)',\n",
       "  '18/12/17 14:18:06 INFO YarnClientSchedulerBackend: Stopped',\n",
       "  '18/12/17 14:18:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!',\n",
       "  '18/12/17 14:18:06 INFO MemoryStore: MemoryStore cleared',\n",
       "  '18/12/17 14:18:06 INFO BlockManager: BlockManager stopped',\n",
       "  '18/12/17 14:18:06 INFO BlockManagerMaster: BlockManagerMaster stopped',\n",
       "  '18/12/17 14:18:06 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!',\n",
       "  '18/12/17 14:18:06 INFO SparkContext: Successfully stopped SparkContext',\n",
       "  '18/12/17 14:18:07 INFO ShutdownHookManager: Shutdown hook called',\n",
       "  '18/12/17 14:18:07 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-9106cee7-d23c-42fb-afb1-6baaf8b4585d',\n",
       "  '18/12/17 14:18:07 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-5cb00a9f-66c6-4ca0-b944-166f7b1030b5',\n",
       "  '18/12/17 14:18:07 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-5cb00a9f-66c6-4ca0-b944-166f7b1030b5/pyspark-d70c6c5d-1a97-42bc-9b0f-c988e40326f5',\n",
       "  '\\nstderr: ',\n",
       "  '\\nYARN Diagnostics: '])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.logs(job1.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Look at the logs for the second job. Make sure you see this log line:\n",
    "```\n",
    "  'Pi is roughly 3.139560',\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31,\n",
       " 42,\n",
       " 142,\n",
       " ['18/12/17 14:17:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, edill); groups with view permissions: Set(); users  with modify permissions: Set(livy, edill); groups with modify permissions: Set()',\n",
       "  '18/12/17 14:17:48 INFO Client: Submitting application application_1544723249474_0021 to ResourceManager',\n",
       "  '18/12/17 14:17:48 INFO YarnClientImpl: Submitted application application_1544723249474_0021',\n",
       "  '18/12/17 14:17:48 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1544723249474_0021 and attemptId None',\n",
       "  '18/12/17 14:17:49 INFO Client: Application report for application_1544723249474_0021 (state: ACCEPTED)',\n",
       "  '18/12/17 14:17:49 INFO Client: ',\n",
       "  '\\t client token: Token { kind: YARN_CLIENT_TOKEN, service:  }',\n",
       "  '\\t diagnostics: AM container is launched, waiting for AM container to Register with RM',\n",
       "  '\\t ApplicationMaster host: N/A',\n",
       "  '\\t ApplicationMaster RPC port: -1',\n",
       "  '\\t queue: default',\n",
       "  '\\t start time: 1545056268662',\n",
       "  '\\t final status: UNDEFINED',\n",
       "  '\\t tracking URL: http://ip-172-31-20-241.ec2.internal:20888/proxy/application_1544723249474_0021/',\n",
       "  '\\t user: edill',\n",
       "  '18/12/17 14:17:50 INFO Client: Application report for application_1544723249474_0021 (state: ACCEPTED)',\n",
       "  '18/12/17 14:17:51 INFO Client: Application report for application_1544723249474_0021 (state: ACCEPTED)',\n",
       "  '18/12/17 14:17:52 INFO Client: Application report for application_1544723249474_0021 (state: ACCEPTED)',\n",
       "  '18/12/17 14:17:53 INFO Client: Application report for application_1544723249474_0021 (state: ACCEPTED)',\n",
       "  '18/12/17 14:17:54 INFO Client: Application report for application_1544723249474_0021 (state: ACCEPTED)',\n",
       "  '18/12/17 14:17:55 INFO Client: Application report for application_1544723249474_0021 (state: ACCEPTED)',\n",
       "  '18/12/17 14:17:56 INFO Client: Application report for application_1544723249474_0021 (state: ACCEPTED)',\n",
       "  '18/12/17 14:17:57 INFO Client: Application report for application_1544723249474_0021 (state: ACCEPTED)',\n",
       "  '18/12/17 14:17:58 INFO Client: Application report for application_1544723249474_0021 (state: ACCEPTED)',\n",
       "  '18/12/17 14:17:59 INFO Client: Application report for application_1544723249474_0021 (state: ACCEPTED)',\n",
       "  '18/12/17 14:18:00 INFO Client: Application report for application_1544723249474_0021 (state: ACCEPTED)',\n",
       "  '18/12/17 14:18:01 INFO Client: Application report for application_1544723249474_0021 (state: ACCEPTED)',\n",
       "  '18/12/17 14:18:02 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-31-20-241.ec2.internal, PROXY_URI_BASES -> http://ip-172-31-20-241.ec2.internal:20888/proxy/application_1544723249474_0021), /proxy/application_1544723249474_0021',\n",
       "  '18/12/17 14:18:02 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter',\n",
       "  '18/12/17 14:18:02 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)',\n",
       "  '18/12/17 14:18:02 INFO Client: Application report for application_1544723249474_0021 (state: RUNNING)',\n",
       "  '18/12/17 14:18:02 INFO Client: ',\n",
       "  '\\t client token: Token { kind: YARN_CLIENT_TOKEN, service:  }',\n",
       "  '\\t diagnostics: N/A',\n",
       "  '\\t ApplicationMaster host: 172.31.28.18',\n",
       "  '\\t ApplicationMaster RPC port: 0',\n",
       "  '\\t queue: default',\n",
       "  '\\t start time: 1545056268662',\n",
       "  '\\t final status: UNDEFINED',\n",
       "  '\\t tracking URL: http://ip-172-31-20-241.ec2.internal:20888/proxy/application_1544723249474_0021/',\n",
       "  '\\t user: edill',\n",
       "  '18/12/17 14:18:02 INFO YarnClientSchedulerBackend: Application application_1544723249474_0021 has started running.',\n",
       "  \"18/12/17 14:18:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39177.\",\n",
       "  '18/12/17 14:18:02 INFO NettyBlockTransferService: Server created on ip-172-31-20-241.ec2.internal:39177',\n",
       "  '18/12/17 14:18:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy',\n",
       "  '18/12/17 14:18:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-31-20-241.ec2.internal, 39177, None)',\n",
       "  '18/12/17 14:18:02 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-20-241.ec2.internal:39177 with 414.4 MB RAM, BlockManagerId(driver, ip-172-31-20-241.ec2.internal, 39177, None)',\n",
       "  '18/12/17 14:18:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-31-20-241.ec2.internal, 39177, None)',\n",
       "  '18/12/17 14:18:02 INFO BlockManager: external shuffle service port = 7337',\n",
       "  '18/12/17 14:18:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-31-20-241.ec2.internal, 39177, None)',\n",
       "  '18/12/17 14:18:03 INFO EventLoggingListener: Logging events to hdfs:/var/log/spark/apps/application_1544723249474_0021',\n",
       "  '18/12/17 14:18:03 INFO Utils: Using initial executors = 0, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances',\n",
       "  '18/12/17 14:18:03 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8',\n",
       "  '18/12/17 14:18:03 INFO SparkContext: Starting job: reduce at /mnt/tmp/spark-0cd00fe6-c60c-4f26-ba2b-b044798ba7d0/pi.py:48',\n",
       "  '18/12/17 14:18:03 INFO DAGScheduler: Got job 0 (reduce at /mnt/tmp/spark-0cd00fe6-c60c-4f26-ba2b-b044798ba7d0/pi.py:48) with 2 output partitions',\n",
       "  '18/12/17 14:18:03 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at /mnt/tmp/spark-0cd00fe6-c60c-4f26-ba2b-b044798ba7d0/pi.py:48)',\n",
       "  '18/12/17 14:18:03 INFO DAGScheduler: Parents of final stage: List()',\n",
       "  '18/12/17 14:18:03 INFO DAGScheduler: Missing parents: List()',\n",
       "  '18/12/17 14:18:03 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[1] at reduce at /mnt/tmp/spark-0cd00fe6-c60c-4f26-ba2b-b044798ba7d0/pi.py:48), which has no missing parents',\n",
       "  '18/12/17 14:18:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.4 KB, free 414.4 MB)',\n",
       "  '18/12/17 14:18:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.0 KB, free 414.4 MB)',\n",
       "  '18/12/17 14:18:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-20-241.ec2.internal:39177 (size: 3.0 KB, free: 414.4 MB)',\n",
       "  '18/12/17 14:18:04 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1079',\n",
       "  '18/12/17 14:18:04 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[1] at reduce at /mnt/tmp/spark-0cd00fe6-c60c-4f26-ba2b-b044798ba7d0/pi.py:48) (first 15 tasks are for partitions Vector(0, 1))',\n",
       "  '18/12/17 14:18:04 INFO YarnScheduler: Adding task set 0.0 with 2 tasks',\n",
       "  '18/12/17 14:18:04 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1)',\n",
       "  '18/12/17 14:18:09 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.28.18:55478) with ID 1',\n",
       "  '18/12/17 14:18:09 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 1)',\n",
       "  '18/12/17 14:18:09 WARN TaskSetManager: Stage 0 contains a task of very large size (371 KB). The maximum recommended task size is 100 KB.',\n",
       "  '18/12/17 14:18:09 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, ip-172-31-28-18.ec2.internal, executor 1, partition 0, PROCESS_LOCAL, 380245 bytes)',\n",
       "  '18/12/17 14:18:09 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, ip-172-31-28-18.ec2.internal, executor 1, partition 1, PROCESS_LOCAL, 508048 bytes)',\n",
       "  '18/12/17 14:18:09 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-28-18.ec2.internal:40521 with 2.8 GB RAM, BlockManagerId(1, ip-172-31-28-18.ec2.internal, 40521, None)',\n",
       "  '18/12/17 14:18:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-28-18.ec2.internal:40521 (size: 3.0 KB, free: 2.8 GB)',\n",
       "  '18/12/17 14:18:11 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1434 ms on ip-172-31-28-18.ec2.internal (executor 1) (1/2)',\n",
       "  '18/12/17 14:18:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1459 ms on ip-172-31-28-18.ec2.internal (executor 1) (2/2)',\n",
       "  '18/12/17 14:18:11 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool ',\n",
       "  '18/12/17 14:18:11 INFO DAGScheduler: ResultStage 0 (reduce at /mnt/tmp/spark-0cd00fe6-c60c-4f26-ba2b-b044798ba7d0/pi.py:48) finished in 7.217 s',\n",
       "  '18/12/17 14:18:11 INFO DAGScheduler: Job 0 finished: reduce at /mnt/tmp/spark-0cd00fe6-c60c-4f26-ba2b-b044798ba7d0/pi.py:48, took 7.290216 s',\n",
       "  'Pi is roughly 3.139560',\n",
       "  '18/12/17 14:18:11 INFO SparkUI: Stopped Spark web UI at http://ip-172-31-20-241.ec2.internal:4042',\n",
       "  '18/12/17 14:18:11 INFO YarnClientSchedulerBackend: Interrupting monitor thread',\n",
       "  '18/12/17 14:18:11 INFO YarnClientSchedulerBackend: Shutting down all executors',\n",
       "  '18/12/17 14:18:11 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down',\n",
       "  '18/12/17 14:18:11 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices',\n",
       "  '(serviceOption=None,',\n",
       "  ' services=List(),',\n",
       "  ' started=false)',\n",
       "  '18/12/17 14:18:11 INFO YarnClientSchedulerBackend: Stopped',\n",
       "  '18/12/17 14:18:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!',\n",
       "  '18/12/17 14:18:11 INFO MemoryStore: MemoryStore cleared',\n",
       "  '18/12/17 14:18:11 INFO BlockManager: BlockManager stopped',\n",
       "  '18/12/17 14:18:11 INFO BlockManagerMaster: BlockManagerMaster stopped',\n",
       "  '18/12/17 14:18:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!',\n",
       "  '18/12/17 14:18:11 INFO SparkContext: Successfully stopped SparkContext',\n",
       "  '18/12/17 14:18:12 INFO ShutdownHookManager: Shutdown hook called',\n",
       "  '18/12/17 14:18:12 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-0cd00fe6-c60c-4f26-ba2b-b044798ba7d0',\n",
       "  '18/12/17 14:18:12 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-3ea09a86-5c0e-4ff7-bf7f-082fb6ed266d',\n",
       "  '18/12/17 14:18:12 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-3ea09a86-5c0e-4ff7-bf7f-082fb6ed266d/pyspark-eade02be-42a7-4c20-82df-7e03f9402fba',\n",
       "  '\\nstderr: ',\n",
       "  '\\nYARN Diagnostics: '])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.logs(job2.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now, let's start a third job so we can kill it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'edill-pi', 'file': '/user/edill/pi.py'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batch(id=32, appId=None, appInfo={'driverLogUrl': None, 'sparkUiUrl': None}, log=[see self.log], state=starting)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job3 = api.submit(file=file_on_hdfs, name=job_name)\n",
    "job3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's kill it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'msg': 'deleted'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.kill(job3.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see this above\n",
    "\n",
    "```\n",
    "{'msg': 'deleted'}\n",
    "```\n",
    "\n",
    "Then things are working as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "livy-submit-dev",
   "language": "python",
   "name": "livy-submit-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
